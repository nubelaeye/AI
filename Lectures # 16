# In this lecture we will study about the Eigenvectors and Eigenvalues.

# Eigenvectors:- is a special vector v such that when it is transformed by some matrix, the 
# product has the exact same direction as v.

# Eigenvalues:- is a scalar that simply scales the eigenvector v such that the following 
# equation is satisfied : Av = lambda of v.

# Ok, then let's code it up!
import numpy as np 

A = np.array([[-1, 4],[2,-2]])

lambdas, v = np.linalg.eig(A)
# printing the eigenvectors each columns is a separate eigenvector 1, 2 and both have its 
print(v)
# own eigenvalues which is represented here!
# printing the eigenvalues
print(lambdas)

# Checking that the A of v is equal to the Lambda of v.
V = v[:,0]

Lbd = lambdas[0]

Av = np.dot(A,V)
print(Av)
# By these both we know the left hand is equal to the right hand: A * v = lambda * v.
print(Lbd * V)

# Eigenvector in greater than 2 Dimensional!
x = np.array([[25,2,9],[5,26,-5],[3,7,-1]])
lambdaX, V_X = np.linalg.eig(x)

# One eigenvector per column of x..

print(V_X)  
# A corresponding eigenvalues for each eigenvectors..

print(lambdaX)

# X * v = lambda * v
v_x = V_X[:,0]

lambda_x = lambdaX[0]

print(np.dot(x,v_x))

print(v_x * lambda_x)

# At the very last the best explanation and differentiation between Eigenvalues & Eigenvector.
# Eigenvalues and eigenvectors are fundamental concepts in linear algebra, often used in various 
# fields such as physics, engineering, computer science,
# and data analysis. Let's break down these concepts in a simple way:

# Eigenvalues (λ):

# Eigenvalues are scalars (single numbers) associated with a square matrix 
# (a matrix with the same number of rows and columns).
# They represent how the matrix scales or stretches space in certain directions.
# For a matrix A and an eigenvalue λ, if there exists a non-zero vector v such that 
# when A operates on v (i.e., Av), the result is a scaled version of v, then λ is an 
# eigenvalue of A.
# Mathematically, for a square matrix A and an eigenvalue λ, we have the equation: Av = λv, 
# where v is the eigenvector corresponding to λ.
# Eigenvectors (v):

# Eigenvectors are non-zero vectors associated with eigenvalues.
# They represent the directions along which the linear transformation (represented by the matrix) 
# only results in a scaling of the vector. Eigenvectors are often normalized to have a magnitude 
# of 1 for simplicity, which means they represent direction only, not magnitude.
# In summary, eigenvalues tell you how a matrix scales space in certain directions, 
# while eigenvectors represent those directions. 
# When you multiply a matrix by an eigenvector, the result is a scaled version of that eigenvector,
# where the scale factor is the corresponding eigenvalue.
